<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory">
  <meta name="keywords" content="Vision-Language-Navigation, Open Vocabulary Object Detector">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory</title>

  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-TSPQB2LZ');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/TRAIL_BLACK_ICON.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TSPQB2LZ" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.trailab.utias.utoronto.ca/mingfeng-yuan">Mingfeng Yuan</a>,</span>
              <span class="author-block">
                <a href="https://letianwang0.wixsite.com/myhome">Letian Wang</a>,</span>
              <span class="author-block">
                <a href="https://www.trailab.utias.utoronto.ca/steven-waslander">Steven Waslander</a>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">University of Toronto</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">IROS 2025, Hangzhou, China</span>
            </div>
            
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=11247593&tag=1"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="http://arxiv.org/abs/2507.18033" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
               
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/TRAILab/OpenNav-website/tree/main" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code is Coming</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

    <section class="section">
    <!div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Key Features</h2>
          <div class="content has-text-justified">
            <ul>
              <li><strong>Open-World Outdoor Navigation (OpenNav)</strong> is the first framework to enable zero-shot outdoor navigation by directly generating trajectories with an MLLM through a single task-agnostic prompt, without pre-trained skills, motion primitives, or in-context examples, allowing navigation with open-set instructions and objects.
              <li><strong>Multi-Expert System for Robust Scene Comprehension</strong> By integrating state-of-the-art MLLMs with an open-vocabulary perception system (OVPS), OpenNav enhances environmental perception granularity, ensuring accurate interpretation of free-form language instructions while maintaining robustness against detector misdetections.
              <li><strong>Turning Language Instructions into Robot Actions (Trajectory Level).</strong> OpenNav combines the reasoning, code generation, and function-calling abilities of MLLMs with classical planning techniques, harnessing the benefits of both human-like reasoning and geometry-compliant trajectory synthesis.
              <li><strong>Evaluation in Autonomous Vehicle Datasets (AVDs)</strong>: We validate OpenNav's performance using AVDs, offering a new approach to studying embodied intelligence with rich, labeled data from real-world navigation tasks.
            </ul>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
  </section>

  <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="has-text-centered">
        <h2 class="title is-3">ðŸŽ¥ OpenNav Demo Videos (Hardware)</h2>
        <div class="columns is-multiline is-centered">
          <div class="column is-half">
            <iframe width="100%" height="315" src="https://www.youtube.com/embed/fgQlC5QFyss?si=fGHlkeNblcxoB0nr" 
              frameborder="0" allowfullscreen></iframe>
          </div>
                  
          <div class="column is-half">
            <iframe width="100%" height="315" src="https://www.youtube.com/embed/9ob8ZIfhDW0?si=ryExccAWzh8ejS3u" 
              frameborder="0" allowfullscreen></iframe>
          </div>
          
          <div class="column is-half">
            <iframe width="100%" height="315" src="https://www.youtube.com/embed/_auKNIeF21s?si=AXZgigi6R3uOMWxm" 
              frameborder="0" allowfullscreen></iframe>
          </div>
          
          <div class="column is-half">
            <iframe width="100%" height="315" src="https://www.youtube.com/embed/gWAnhbbDyIU?si=gwl4CglGTJlZASJU" 
              frameborder="0" allowfullscreen></iframe>
          </div>
                    
          <div class="column is-half">
            <iframe width="100%" height="315" src="https://www.youtube.com/embed/2WHk9VD6wIo?si=HqAQkod1XmBBml2I" 
              frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
              allowfullscreen></iframe>
          </div>
          
          <div class="column is-half">
            <iframe width="100%" height="315" src="https://www.youtube.com/embed/cJqdAz9OweU?si=jaBt0-8mQ8hJq7nb" 
              frameborder="0" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 1em;">ðŸ§  Method Overview</h2>
      </div>
      
      <div class="has-text-centered" style="margin-top: 1em;">
          <!-- <embed src="./static/images/JDT3D_Architecture_v6.pdf" style="width: 80%; height: 500px;" alt="JDT3D Architecture"> -->
            <img src="./static/images/Framework2.jpg" style="width: 95%;" alt="OpenNav Architecture">
        </div>
          <h2 class="subtitle" style="text-align: justify;">
         Overview of <strong>OpenNav.</strong> Given the posed RGB-Lidar observation of the environment and an open-set free-form language instruction, 1) we leverages task-agnostic prompts to enable zero-shot generalization and adaptability to varied instructions; 2) MLLM generates code, which interacts with OVPS, to produce open-set multimodal scene perception outputs, and 2D bird-eye-view (BEV) value map (consists of a semantic map and occupancy map) grounded in the operation environment. 3) MLLM synthesizes a human-like coarse trajectory based on instructions, scene understanding, and its reasoning capabilities. The generated BEV value map then serves as the objective function for the motion planner, which refines the trajectory to ensure geometry-compliant navigation. Please see the next Figure for detailed pipeline, inputs, and outputs of the OVPS.
        </h2>
      </div>
    </div>
  </section>

    <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="has-text-centered">
          <!-- <embed src="./static/images/JDT3D_Architecture_v6.pdf" style="width: 80%; height: 500px;" alt="JDT3D Architecture"> -->
            <img src="./static/images/Perception.jpg" style="width: 95%;" alt="OVPS Architecture">
        </div>
        <h2 class="subtitle" style="text-align: justify;">
          Overview of <span class="dnerf"> Open Vocabulary Perception System (OVPS).</span> OVPS sequentially performs detection, segmentation, and object caption generation. Combined with 3D point clouds, the system will generate 1) multimodal observations for VLN, including text prompts and image prompts for MLLMs, 2) 3D reconstructed map, as well as 2D occupancy and semantic maps for trajectory refinement.
        </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 1em;">ðŸš˜ Language Guided Zero-Shot Trajectory Synthesis on AVDs</h2>
      </div>
      
      <div class="has-text-centered" style="margin-top: 1em;">
          <!-- <embed src="./static/images/JDT3D_Architecture_v6.pdf" style="width: 80%; height: 500px;" alt="JDT3D Architecture"> -->
            <img src="./static/images/Selection.png" style="width: 95%;" alt="OpenNav Demo">
        </div>
        <h2 class="subtitle" style="text-align: justify;">
          Given a free-form language instruction and sensor observations, OpenNav is capable of generating a dense sequence of instruction-following and scene-compliant robot waypoints in a zero-shot manner for open-world navigation, effectively handling open-set objects and open-set instructions without relying on in-context examples or pre-trained skills.
        </h2>
      </div>
    </div>
  </section>

<section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="has-text-centered">
          <!-- <embed src="./static/images/JDT3D_Architecture_v6.pdf" style="width: 80%; height: 500px;" alt="JDT3D Architecture"> -->
            <img src="./static/images/Language guided 3 results.png" style="width: 95%;" alt="OpenNav Demo">
        </div>
        <h2 class="subtitle" style="text-align: justify;">
          Selected examples demonstrating how OpenNav utilizes Value Maps to generate task-aligned and geometry-compliant navigation trajectories: <strong>Left:</strong> User-specified tasks and trajectories generated by OpenNav; <strong>Right:</strong> Value maps illustrating trajectories generated by different algorithms based on the given tasks.
        </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="has-text-centered">
          <!-- <embed src="./static/images/JDT3D_Architecture_v6.pdf" style="width: 80%; height: 500px;" alt="JDT3D Architecture"> -->
            <img src="./static/images/Trajectory demo.png" style="width: 95%;" alt="OpenNav Demo">
        </div>
        <h2 class="subtitle" style="text-align: justify;">
          This demo illustrates how language instructions update the value map through the occupancy map and semantic map to generate instruction-aligned trajectories. In the <strong>first</strong> task, the instruction is to move straight for 20 meters, where flat road areas have a lower cost. In the <strong>second</strong> task, the instruction requires avoiding white-lettered areas on the ground, resulting in these regions being assigned a higher cost. In the <strong>third</strong> task, the instruction specifies avoiding shaded sidewalk areas, which are also assigned a higher cost accordingly.
        </h2>
      </div>
    </div>
  </section>
  
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@INPROCEEDINGS{11247593,
  author={Yuan, Mingfeng and Wang, Letian and Waslander, Steven L.},
  booktitle={2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={OpenNav: Open-World Navigation with Multimodal Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={18948-18955},
  doi={10.1109/IROS60139.2025.11247593}}
</code></pre>
    </div>
  </section>
  
  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/TRAILab" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
            We would like to express our gratitude to the open-source projects and their contributors, especially <a href="https://github.com/BIT-DYN/OpenGraph">OpenGraph</a>. Their valuable work has greatly contributed to the development of our codebase.
            </p>
            <p>
              Thank you to the authors of <a href="https://github.com/nerfies/nerfies.github.io/tree/main">Nerfies</a> for the website template.
            </p>
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
